"""Summarize using both LSA and the spacy parsing library."""
import logging
from operator import length_hint
import math
import numpy
from numpy.linalg import svd
import spacy
from spacy.parts_of_speech import VERB, NOUN

from summarizer.model.base_summarizer import BaseSummarizer, normalize_word

logging.basicConfig(level=logging.DEBUG)
_logger = logging.getLogger(__name__)  # pylint: disable=invalid-name


class LsaSummarizer(BaseSummarizer):
    """Summarize using LSA."""

    MIN_DIMENSIONS = 3
    REDUCTION_RATIO = 1 / 1
    QUESTION_WORDS = frozenset({u'can', u'should', u'will', u'could', u'why', u'what', u'how', u'is'})
    _stop_words = frozenset()

    def __init__(self, ):
        BaseSummarizer.__init__(self, )
        self.nlp = spacy.load('en')
        self.nlp_doc = None
        self.user_dict = None

    @property
    def stop_words(self):
        """Get the stopwords."""
        return self._stop_words

    @stop_words.setter
    def stop_words_setter(self, words):
        """Generate a cononical stopwords set."""
        self._stop_words = frozenset(map(normalize_word, words))

    def __call__(self, document, sentences_count, user_dict):  # pylint: disable=invalid-name
        """Run the LSA summarizer."""
        self.nlp_doc = self.nlp(document)
        self.user_dict = user_dict
        _logger.info('Created doc')

        dictionary = self._create_dictionary()
        # empty document
        if not dictionary:
            return []
        matrix = self._create_matrix(dictionary)
        tf_matrix = _compute_term_frequency(matrix)
        _, sigma, v_matrix = svd(tf_matrix, full_matrices=False)
        ranks = iter(self._compute_ranks(sigma, v_matrix))
        sents = [s.text for s in self.nlp_doc.sents]
        _logger.info('Sentences generated by spacy are %s, count %s', sents, len(sents))
        new_sents = self.get_best_sentences(sents, sentences_count * 2, lambda s: next(ranks))
        filt_sents = [sent for sent in new_sents if self.better_question(sent)]
        additional_sents = set(new_sents) - set(filt_sents)
        to_add = sentences_count - len(filt_sents)
        final_sents = filt_sents
        if to_add > 0:
            final_sents += sorted(list(additional_sents)[:to_add], key=length_hint, reverse=True)
        _logger.info('Filtered sentences %s', filt_sents)
        _logger.info('Final recommendations are %s', final_sents[:sentences_count])
        return final_sents

    def better_question(self, txt):
        """Normalize txt in what is believed to be a question."""
        if len(txt.split()) > 5:
            parse = self.nlp(txt)
            for sent in parse.sents:
                if len(sent) > 5:
                    for (i, word) in enumerate(sent):
                        if word.lemma_ in self.QUESTION_WORDS:
                            return u'ROOT' in [x.dep_ for x in sent[i + 1:]] and u'?' in [x.orth_ for x in sent[i + 1:]]
        return False

    def _create_dictionary(self, ):
        """Create mapping key = word, value = row index."""
        unique_words = frozenset(
            w.lemma_ for w in self.nlp_doc if not w.is_stop and w.tag_ != "PRP" and (w.pos == VERB or w.pos == NOUN))
        unique_users = frozenset(self.user_dict.values())
        _logger.info('Have %s unique words', len(unique_words))
        _logger.info('Have %s unique users', len(unique_users))
        return dict((w, i) for i, w in enumerate(unique_words | unique_users))

    def _create_matrix(self, dictionary):
        """Create matrix of shape |unique words|×|sentences|.

        Cells contain number of occurrences of words (rows) in sentences (cols).
        """
        sentences = list(self.nlp_doc.sents)
        words_count = len(dictionary)
        sentences_count = len(sentences)
        _logger.info('Have %s sentences ', sentences_count)
        if words_count < sentences_count:
            _logger.warning('Number of words %s is lower than number of sentences %s.', words_count, sentences_count)
            _logger.warning('LSA may not work properly')
        # create matrix |unique words|×|sentences| filled with zeroes
        matrix = numpy.zeros((words_count, sentences_count))
        for col, sentence in enumerate(sentences):
            for word in [wd.lemma_ for wd in sentence if wd.lemma_ in dictionary]:
                matrix[dictionary[word], col] += 1
            if sentence.text in self.user_dict and len(self.user_dict[sentence.text]) > 1:
                _logger.info("Matching sentence %s with user %s", sentence.text, self.user_dict[sentence.text])
                matrix[dictionary[self.user_dict[sentence.text]], col] += 1
        return matrix

    def _compute_ranks(self, sigma, v_matrix):
        assert len(sigma) == v_matrix.shape[0], 'Matrices should be multiplicable'

        dimensions = max(self.MIN_DIMENSIONS,
                         int(len(sigma) * self.REDUCTION_RATIO))
        powered_sigma = tuple(s ** 2 if i < dimensions else 0.0
                              for i, s in enumerate(sigma))
        ranks = []
        # iterate over columns of matrix (rows of transposed matrix)
        for column_vector in v_matrix.T:
            rank = sum(s * v ** 2 for s, v in zip(powered_sigma, column_vector))
            ranks.append(math.sqrt(rank))
        return ranks


def _compute_term_frequency(matrix, smooth=0.4):
    """Compute TF metrics for each sentence (column) in the given matrix."""
    assert 0.0 <= smooth < 1.0
    max_word_frequencies = numpy.max(matrix, axis=0)
    rows, cols = matrix.shape
    for row in range(rows):
        for col in range(cols):
            max_word_frequency = max_word_frequencies[col]
            if max_word_frequency != 0:
                frequency = matrix[row, col] / max_word_frequency
                matrix[row, col] = smooth + (1.0 - smooth) * frequency
    return matrix
